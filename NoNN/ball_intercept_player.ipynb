{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_comp(prev_frame, curr_frame, num_points=500, points_to_use=500, transform_type='affine'):\n",
    "    \"\"\" Obtains new warped frame1 to account for camera (ego) motion\n",
    "        Inputs:\n",
    "            prev_frame - first image frame\n",
    "            curr_frame - second sequential image frame\n",
    "            num_points - number of feature points to obtain from the images\n",
    "            points_to_use - number of point to use for motion translation estimation \n",
    "            transform_type - type of transform to use: either 'affine' or 'homography'\n",
    "        Outputs:\n",
    "            A - estimated motion translation matrix or homography matrix\n",
    "            prev_points - feature points obtained on previous image\n",
    "            curr_points - feature points obtaine on current image\n",
    "        \"\"\"\n",
    "    transform_type = transform_type.lower()\n",
    "    assert(transform_type in ['affine', 'homography'])\n",
    "\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)\n",
    "    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # get features for first frame\n",
    "    corners = cv2.goodFeaturesToTrack(prev_gray, num_points, qualityLevel=0.01, minDistance=10)\n",
    "\n",
    "    # get matching features in next frame with Sparse Optical Flow Estimation\n",
    "    matched_corners, status, _ = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, corners, None)\n",
    "\n",
    "    # reformat previous and current corner points\n",
    "    prev_points = corners[status==1]\n",
    "    curr_points = matched_corners[status==1]\n",
    "\n",
    "    # sub sample number of points so we don't overfit\n",
    "    if points_to_use > prev_points.shape[0]:\n",
    "        points_to_use = prev_points.shape[0]\n",
    "\n",
    "    index = np.random.choice(prev_points.shape[0], size=points_to_use, replace=False)\n",
    "    prev_points_used = prev_points[index]\n",
    "    curr_points_used = curr_points[index]\n",
    "\n",
    "    # find transformation matrix from frame 1 to frame 2\n",
    "    if transform_type == 'affine':\n",
    "        A, _ = cv2.estimateAffine2D(prev_points_used, curr_points_used, method=cv2.RANSAC)\n",
    "    elif transform_type == 'homography':\n",
    "        A, _ = cv2.findHomography(prev_points_used, curr_points_used)\n",
    "\n",
    "    return A, prev_points, curr_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "\n",
    "def player_pipeline(path):\n",
    "    # frame_files = sorted([f for f in os.listdir(path) if f.endswith('.jpg')])\n",
    "\n",
    "\n",
    "    # first_frame_path = os.path.join(path, frame_files[0])\n",
    "    # prev = cv2.imread(first_frame_path)\n",
    "    \n",
    "    #get frames from video\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    ret, prev = cap.read()\n",
    "    print(ret)\n",
    "\n",
    "    while ret:\n",
    "        \n",
    "        # Read the frame\n",
    "        #frame_path = os.path.join(path, frame_file)\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        A, prev_points, curr_points = motion_comp(prev, frame, num_points=10000, points_to_use=10000, transform_type='affine')\n",
    "        transformed1 = cv2.warpAffine(prev, A, dsize=(prev.shape[:2][::-1]))\n",
    "        comped_delta = cv2.subtract(frame, transformed1)\n",
    "        comped_delta = cv2.cvtColor(comped_delta, cv2.COLOR_BGR2GRAY)\n",
    "        ret, thresh = cv2.threshold(comped_delta, 70, 250, cv2.THRESH_BINARY)\n",
    "        dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
    "\n",
    "        numLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(dilated, connectivity=8, ltype=cv2.CV_32S)\n",
    "        \n",
    "        if len(centroids)>1:\n",
    "\n",
    "            # Perform hierarchical clustering on component centers\n",
    "            clusterer = AgglomerativeClustering(n_clusters=None, distance_threshold=52, linkage='single')\n",
    "            clusters = clusterer.fit_predict(centroids)\n",
    "\n",
    "            # Draw bounding boxes around clustered component centers\n",
    "            output = frame.copy()\n",
    "            #create a data structure to store the bounding boxes\n",
    "            bounding_boxes = []\n",
    "            for cluster_id in np.unique(clusters):\n",
    "                cluster_indices = np.where(clusters == cluster_id)[0]\n",
    "                cluster_centers = centroids[cluster_indices]\n",
    "                min_x = int(np.min(cluster_centers[:, 0]))\n",
    "                min_y = int(np.min(cluster_centers[:, 1]))\n",
    "                max_x = int(np.max(cluster_centers[:, 0]))\n",
    "                max_y = int(np.max(cluster_centers[:, 1]))\n",
    "\n",
    "                # Calculate area of the bounding box\n",
    "                area = (max_x - min_x) * (max_y - min_y)\n",
    "                \n",
    "\n",
    "                if area > 1000:\n",
    "                    cv2.rectangle(output, (min_x - 5, min_y - 5), (max_x + 5, max_y + 5), (255, 0, 0), 2)\n",
    "                    #store bounding boxes in the data array\n",
    "                    bounding_boxes.append([min_x, min_y, max_x, max_y])\n",
    "                    # Draw center\n",
    "                    center_x, center_y = (min_x + max_x) // 2, (min_y + max_y) // 2\n",
    "                    cv2.circle(output, (center_x, center_y), 4, (0, 255, 0), -1)\n",
    "\n",
    "                    # Add label\n",
    "                    cv2.putText(output, 'Player', (min_x, min_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow(\"Output\", output)\n",
    "        else:\n",
    "            cv2.imshow(\"Output\", frame)\n",
    "        prev = frame.copy()\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /Users/xperience/GHA-OpenCV-Python2/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplayer_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/Gloria/Desktop/LABS/TennisBounceDetector/videoin/veryshorttest.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 21\u001b[0m, in \u001b[0;36mplayer_pipeline\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m ret:\n\u001b[1;32m     16\u001b[0m     \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Read the frame\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#frame_path = os.path.join(path, frame_file)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 21\u001b[0m     A, prev_points, curr_points \u001b[38;5;241m=\u001b[39m \u001b[43mmotion_comp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints_to_use\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maffine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     transformed1 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpAffine(prev, A, dsize\u001b[38;5;241m=\u001b[39m(prev\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     23\u001b[0m     comped_delta \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39msubtract(frame, transformed1)\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mmotion_comp\u001b[0;34m(prev_frame, curr_frame, num_points, points_to_use, transform_type)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(transform_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffine\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhomography\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m prev_gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(prev_frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2GRAY)\n\u001b[0;32m---> 18\u001b[0m curr_gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_RGB2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# get features for first frame\u001b[39;00m\n\u001b[1;32m     21\u001b[0m corners \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mgoodFeaturesToTrack(prev_gray, num_points, qualityLevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, minDistance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /Users/xperience/GHA-OpenCV-Python2/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "player_pipeline('/Users/Gloria/Desktop/LABS/TennisBounceDetector/videoin/veryshorttest.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
